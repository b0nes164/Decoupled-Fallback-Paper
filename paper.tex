%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,sigconf')
%%
%% IMPORTANT NOTICE:
%%
%% For the copyright see the source file.
%%
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%%
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%%
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro~\cite [option:text,text]
%TC:macro~\citep [option:text,text]
%TC:macro~\citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.

\documentclass[sigconf]{acmart}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

\setcopyright{acmlicensed}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{XXXXXXX.XXXXXXX}
\acmConference[Conference acronym 'XX]{Make sure to enter the correct
  conference title from your rights confirmation emai}{June 03--05,
  2018}{Woodstock, NY}
\acmISBN{978-1-4503-XXXX-X/18/06}

\usepackage[ruled,vlined]{algorithm2e}
\usepackage{amsmath}
\usepackage{mathtools}

\begin{document}

\title{The Name of the Title Is Hope}

\author{Lars Th{\o}rv{\"a}ld}
\affiliation{%
\institution{The Th{\o}rv{\"a}ld Group}
\city{Hekla}
\country{Iceland}}
\email{larst@affiliation.org}

\author{Valerie B\'eranger}
\affiliation{%
  \institution{Inria Paris-Rocquencourt}
  \city{Rocquencourt}
  \country{France}
}

\author{Aparna Patel}
\affiliation{%
  \institution{Rajiv Gandhi University}
  \city{Doimukh}
  \state{Arunachal Pradesh}
  \country{India}}

\renewcommand{\shortauthors}{Trovato et al.}

\begin{abstract}
  TODO
\end{abstract}

\begin{CCSXML}
  <ccs2012>
  <concept>
  <concept_id>00000000.0000000.0000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>500</concept_significance>
  </concept>
  <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>300</concept_significance>
  </concept>
  <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>100</concept_significance>
  </concept>
  <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>100</concept_significance>
  </concept>
  </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
\ccsdesc[300]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
\ccsdesc{Do Not Use This Code~Generate the Correct Terms for Your Paper}
\ccsdesc[100]{Do Not Use This Code~Generate the Correct Terms for Your Paper}

\keywords{Do, Not, Us, This, Code, Put, the, Correct, Terms, for,
  Your, Paper}

\received{20 February 2007}
\received[revised]{12 March 2009}
\received[accepted]{5 June 2009}

\maketitle

\section{Introduction}

\subsection{Goals of a Scan Implementation}
\begin{itemize}
  \item \textbf{Minimal global memory access:} the scan should minimize global memory access to the theoretical minimum of $2n$.
  \item \textbf{Fully saturates global memory bandwidth:} the scan should be capable of fully saturating the global memory bandwidth.
  \item \textbf{Portability:} the scan should be portable across hardware vendors and architectures. 
\end{itemize}

\section{Background}
\subsection{The GPU Programming Environment(Model?)}
Contemporary GPUs are hierarchically organized, massively parallel processors designed to prioritize throughput over latency. Due to their specialized nature, GPUs function as co-processors to the CPU, requiring GPU programs—known as \emph{kernels} or \emph{shaders}—to be \emph{launched} or \emph{dispatched} heterogeneously by the CPU. Upon launch, a kernel is replicated across independent execution contexts, called threads. These threads are first organized into smaller units known as \emph{subgroups}\footnote{Also referred to as \emph{Warps}, \emph{Waves}, or \emph{SIMD-Groups}.}, which are then grouped into larger units called \emph{workgroups}\footnote{Also referred to as \emph{Cross-Thread-Arrays}, \emph{Thread-Blocks}, \emph{Blocks}, \emph{Thread-Groups}, or \emph{Groups}.}. This thread hierarchy mirrors the GPU memory hierarchy: individual threads access private, high-speed registers; threads within a workgroup share low-latency shared memory\footnote{Also referred to as \emph{Local Data Storage}, \emph{Group-Shared Memory}, (Check metal terminology).}; and all threads have access to high-bandwidth but higher-latency global memory.

While threads within the same workgroup can synchronize with each other using barrier primitives and can communicate through shared memory, a workgroup is a logical unit of execution, not a physical processor. Instead, workgroups are dynamically mapped and scheduled to a GPU's underlying \emph{multiprocessors}. This abstraction enables the kernel to execute on GPUs with varying hardware capabilities, such as different numbers of multiprocessors or multiprocessors with differing execution resources. A single multiprocessor can host multiple workgroups; we define the number of active subgroups on a multiprocessor as the \emph{subgroup occupancy}, and the number of active workgroups across the entire GPU as the \emph{workgroup occupancy}. Generally speaking, high subgroup occupancy is desirable because multiprocessors are specifically designed to context switch between subgroups, keeping execution units busy and masking memory and execution latency.

GPUs exhibit both single-instruction, multiple-thread (SIMT) and single-program, multiple-data (SPMD) behaviors. A multiprocessor schedules and executes threads by subgroup. While it attempts to execute the same instruction across all threads in the subgroup, SIMT, threads within the same subgroup are free to follow their own data/context-dependent execution paths, SPMD. However, this flexibility comes at the cost of subgroup divergence. When threads in a subgroup diverge due to conditional branching or other control flow, each unique path is executed sequentially, but at the full execution width, effectively rendering the operation serial and reducing parallel efficiency.\footnote{The scheduling and execution width of the underlying hardware does not necessarily align with the subgroup width.}

Most relevant to this work is the scheduling behavior of the GPU. Scheduling is divided into a workgroup-level scheduler\footnote{For example, NVIDIA's \emph{GigaThread Engine} or AMD's \emph{Asynchronous Compute Engines}}, which manages kernel launches and maps workgroups to multiprocessors, and the aforementioned subgroup scheduler, responsible for selecting which of the currently resident subgroups for execution. Although contemporary GPU hardware supports workgroup preemption to prioritize real-time tasks or manage multi-process workloads~\cite{}(NVIDIA Volta,AMD RDNA), this capability does not extend to the GPU programming model. Once a workgroup begins execution on a multiprocessor, it must run to completion and cannot context switch. At the subgroup scheduling level, \emph{fairness}—how evenly execution resources are distributed among threads—and \emph{progress guarantees}—ensuring that all threads or subgroups eventually make forward progress—are the most salient issues. As we will discuss in the following section, these factors have profound implications for kernel performance and correctness.

\subsection{The Scan Primitive}
The study of \emph{scan} (\emph{parallel prefix}) networks traces back to the design of carry-lookahead adder circuits and beyond\cite{10.1145/322217.322232, 5219801}. A scan is typically defined on a monoid \( M \), characterized by a binary reduction operator \( \oplus \) and an identity element \( \varepsilon \). The binary operator \( \oplus \) satisfies the closure property \( \forall a, b \in M, \ (a \oplus b) \in M \) and has an identity element \( \exists \varepsilon \in M, \ \forall a \in M, \ \varepsilon \oplus a = a \). Although \( \oplus \) must be associative, it is not necessarily commutative, as demonstrated in structures like the stack monoid\cite{}. In a scan, the result at the \( n \)-th element is the reduction of the preceding subset of elements in the sequence. If the subset includes the \( n \)-th element, it is called \textit{inclusive}; if it excludes the \( n \)-th element, it is called \textit{exclusive}. The most common scan type is the prefix sum, where \( \oplus \) is addition. For example:
\[
  x = [x_1, x_2, x_3, \dots, x_n] \ \ \ \ \ \ \ y = [1, 1, 1, 1, 1] 
\]
\[
  \text{InclusiveScan}(x, \oplus) = [x_1, x_1 \oplus x_2, x_1 \oplus x_2 \oplus x_3, \dots, x_1 \oplus x_2 \oplus \cdots \oplus x_n]
\]
\[
  \text{InclusiveScan}(y, +) = [1, 2, 3, 4, 5]
\]

Due to its significance in circuits and as a fundamental algorithmic primitive, scan enjoys a rich literature in both electrical engineering and computer science~\cite{}. As comprehensive taxonomies of scan networks are well-documented, our focus is on exploring the progression of inter-workgroup (global) scan strategies and their fitness relative to our ideal architecture.

\subsection{Evolution of Inter-Workgroup Scan Architectures}
Prior to the introduction of shared memory, early GPU scan implementations~\cite{Horn, Hensley, Greß, Sengupta} operated directly on global memory and performed a single global scan where each level of depth necessitated a separate kernel launch. The introduction of shared memory and dedicated compute environments resulted in the first implementations to resemble contemporary designs, combining coarse-grained inter-workgroup strategies with fine-grained intra-workgroup strategies. \emph{Work tile} partitioning was introduced to divide inputs into workgroup-consumable chunks that fit within the limited size of shared memory. However, as a natural result of this partitioning, a serial dependency is created between the work tiles. In this section, we review the three prominent global scan strategies—\emph{Scan-then-Propagate}, \emph{Reduce-then-Scan}, and \emph{Chained-Scan}—examining how they navigate this inter-workgroup dependency and evaluating their effectiveness in minimizing global memory access and maximizing memory bandwidth utilization in alignment with our ideal architecture goals.

\subsubsection{Scan-then-Propagate}
Introduced by Sengupta et al.\cite{10.5555/1280094.1280110}, the \emph{Scan-then-Propagate} \cite{GPUGems3, Sengupta2008} strategy is an intuitive extension of the intra-workgroup scan, comprising three phases: 
\begin{enumerate} 
  \item \textbf{Intra-Workgroup (Local) Scan:} Each workgroup performs an inclusive scan on its assigned work-tile partition of the input data. The results, including both the local scan output and the reduction of each workgroup, are written back to global memory.
  \item \textbf{Spine Scan:} The reductions from all workgroups, collectively referred to as \emph{the spine}, are gathered and scanned to compute the global prefix of the reductions, resolving the dependencies between workgroups; we call the result of this phase the \emph{root}. 
  \item \textbf{Propagation:} The root is propagated back to each workgroup and combined with the local scan results to compute the final scan. 
\end{enumerate}
Without an inter-workgroup barrier primitive, each phase is implemented as a separate kernel launch to ensure a consistent view of the reductions. During the scan operation, each element is read from and written to global memory once during the local scan and again during the propagation phase, contributing a total of $4n$ global memory operations. Although data movement is dominated by the first and third phase, Sengupta et al.'s implementation handles arbitrarily large inputs using recursion. For an input of size $n$ and a work tile of size $t$, this results in a recursive depth of $\lceil \log_t n \rceil$. At each recursive step $k$, $n/t^k$ tiles are processed, leading to a total of $\sum_{k=1}^{\lceil \log_t n \rceil} n/t^k \approx (n - 1)/(t - 1)$ work tiles over all steps. Since each tile moves $4t$ data ($2t$ local scan, $2t$ propagation), the total data movement is $O\left(\frac{4t(n - 1)}{t - 1}\right) = O(4n)$.

\subsubsection{Reduce-then-Scan}
As the design of GPUs settled around the high-throughput but high-latency paradigm, reducing global memory access, rather than compute, became the prevailing goal for algorithms like scan, which are computationally light enough for their compute to be masked by global memory access. This led to a refinement of the \emph{Scan-Then-Propagate} strategy called \emph{Reduce-Then-Scan}\cite{Merrill Grimshaw, Ha and Han, Dotsenko, Breitbart}: by writing only the work tile reduction in the first phase and performing a redundant scan in the third phase, $n$ global data movement can be eliminated. The \emph{Reduce-Then-Scan} strategy operates in three phases:
\begin{enumerate}
  \item \textbf{Work Tile Reduction:} Each workgroup performs a pure reduction on its work tile partition, posting the result into global memory. 
  \item \textbf{Spine Scan:} The reductions from all workgroups are gathered and scanned to produce the root.
  \item \textbf{Intra-Workgroup Scan and Propagation:} Each workgroup performs the local scan on the work tile, incorporating the root as it writes to global memory.
\end{enumerate}
\emph{Reduce-then-Scan} implementations also introduced improvements to the spine phase of the scan, which coalesced around a workgroup-level \emph{raking} technique first applied to inter-workgroup scan by Merrill and Grimshaw~\cite{}. Instead of handling arbitrary inputs recursively, the total workgroup occupancy $o$ is determined, and the input is partitioned into non-overlapping blocks processed serially by each workgroup. This approach limits the recursive depth of the global scan to two and reduces the global memory movement for the spine scan from $O(n/t)$ (incurred by recursion) to $O(3o)$. As a result, kernel launches are limited to three and total global memory movement is reduced to $O(3n + 3o) = O(3n)$.

\subsubsection{Chained-Scan}
Continuing the trend of global data movement optimization, the most recent work on global scan has been the \emph{Chained-Scan} architecture. First introduced by Yan et al. in \emph{StreamScan}, the key innovation of GPU chained scanning lies in its hybridization of parallel and serial strategies: achieving parallelism at the intra-workgroup level while minimizing global data movement through serial scan operations at the inter-workgroup level.

Instead of using kernel launches as inter-workgroup synchronization points, \emph{Chained-Scans} launch a single kernel that assigns work tiles in a serial order to workgroups. This shifts the problem from explicit synchronization to implicit synchronization via the order in which workgroups access global data. Each workgroup processes its assigned tile independently, waiting only for the result of the preceding reductions before continuing. This structure ensures that global memory traffic is minimized, as each element is read and written exactly once, achieving the theoretical minimum of $2n$.

While \emph{StreamScan} successfully reduces per-element device memory accesses to two, it does not consistently achieve the expected throughput of $\text{memoryBandwidth}/2$ (saturate bandwidth?). This limitation arises because, in \emph{StreamScan}, a workgroup cannot post its reduction to global memory until it has ingested the reduction of the preceding workgroup. This sequential dependency creates a bottleneck, constraining the algorithm’s performance to the rate at which reductions can propagate through device memory—a process limited by the relatively high latency of GPU memory.

Building on \emph{StreamScan}, Merrill and Garland introduced the \emph{Decoupled Lookback} technique to address the bottleneck created by these sequential dependencies. Unlike \emph{StreamScan}, where reductions and dependency resolution occur sequentially, \emph{Decoupled Lookback} separates these processes.In \emph{Decoupled Lookback}, each workgroup posts its reduction to device memory and then enters the “lookback” phase. During this phase, the workgroup performs a serial traversal backward through preceding workgroup reductions stored in device memory. To manage inter-workgroup synchronization, bit-packed status flags are used to indicate the state of each workgroup’s reduction.

TODO Cleanup, add O(n) on global data movement during the lookback phase etc.

\subsection{Ideal Scan Architecture}
RTS
More portable
slower
handling arbitrary inputs can be awkward

Chained Scan
Faster
2n data movement + serial ordering faclitates in-situ behavior
naturally extends to arbitrary data
not portable, relies on something called forward progress guarantees

Chained has lots of advantages, it would be great if it was portable.

\subsection{Why does \emph{Chained-Scan} Rely on Forward Progress Guarantees?}
TODO rewrite
The \emph{Chained-Scan} architecture operates in the middle ground between the absence of inter-workgroup synchronization and formal inter-workgroup synchronization support. Rather than relying on explicit synchronization primitives, it leverages a combination of atomic operations and a property of the hardware's workgroup scheduling model known as a \emph{forward progress guarantee}. Atomics enable coherent communication between workgroups, while the \emph{forward progress guarantee} (FPG) ensures that all active workgroups will eventually make progress towards termination, precluding scenarios where a workgroup is scheduled in a manner that starves another workgroup of execution time.

In \emph{Chained-Scan}, workgroups operate within a producer-consumer framework: each workgroup produces the reduction of its assigned work tile, and every workgroup (except the first) must consume the reductions produced by all preceding workgroups to complete the scan. To guarantee that no workgroup begins scanning a tile for which the preceding reduction may be unavailable, \emph{Chained-Scan} assigns work tiles using atomic increment operations rather than assignment based on workgroup index. Once a workgroup acquires its tile, it computes the tile’s reduction, atomically posts the result into global memory, and then waits for preceding reductions to appear in global memory. It is this waiting phase that necessitates FPG. Because workgroups do not execute in physical lockstep (obvious?), some may begin waiting before others have finished their calculations. Without FPG, there is a risk that a workgroup which finishes early could remain indefinitely scheduled in a waiting state, blocking progress by the workgroup responsible for producing the needed reduction and ultimately causing a deadlock.

As an illustrative example, consider a scan operation which requires exactly two workgroups, running on a GPU with a single multiprocessor. While this multiprocessor can host both workgroups, it contains only a single scheduling unit, which lacks FPG. In this scenario, both workgroups launch and acquire their respective work tiles, but the workgroup responsible for \emph{Work Tile 1} finishes first and begins waiting on \emph{Work Tile 0}. Because the scheduler lacks FPG, it continues scheduling \emph{Work Tile 1}, but because there is only a single scheduling unit, \emph{Work Tile 0} is never scheduled and never completes its reduction. As a result, \emph{Work Tile 1} spins indefinitely until \emph{timeout detection and recovery} (TDR) is triggered, crashing the host program in the process.

\subsection{Why is reliance on Forward Progress Guarantees a Portability Problem?}
Although NVIDIA formalized FPG down to the subgroup level beginning with the Volta architecture---and FPG was likely already present at the workgroup level on at least NVIDIA's Fermi and AMD's TeraScale2 architectures~\cite{}---it remains absent on some contemporary hardware, most notably certain ARM-based chips, including Apple's M-series GPUs~\cite{10.1145/3485508}. Our testing shows that, at best, attempting to run \emph{Chained-Scan} without FPG results in mega-bad\footnote{Need to gather data on the distribution of M1 Pro time on Decoupled Lookback instead of the average time. It's also unclear why the test did not trigger Metal's timeout device recovery}, and at worst, risks the afformentioned TDR and subsequent program crash. This risk is further exacerbated by the fact that, no major graphics API currently provides developers an FPG hardware capability query\footnote{To the best of our knowledge D3D12, Vulkan, and Metal do not have a query for FPG}. As a result, when implementing scan operations in a shading environment—where cross-vendor, cross-architecture portability is essential—uncertainty surrounding FPG capability and its catastrophic failure mode compels developers to fall back to the older, slower \emph{Reduce-then-Scan} approach.

\subsection{Earlier Attempts at Portability}
Scalar fallback here.

\section{Design}
\subsection{Goals (These subsection headers can be removed later if necessary)}
This work is guided by two main goals. First, portability: we want to develop a scan implementation that retains the benefits of the \emph{Chained-Scan} architecture but is also suitable for a diverse range of hardware vendors and achitectures, including those without FPG. Second, performance: we aim for near speed-of-light execution to the greatest extent possible allowed by the underlying hardware and programming model. To ground these objectives, we select the WebGPU shading language (WGSL) and the WebGPU standard as our target environment. WGSL is a meta-level shading language which is translated and compiled as necessary for backend graphics APIs---D3D12, Vulkan, and Metal---and as such, WGSL implementations are bound by the minimum capability across all backends. Because WGSL must operate within this limited capability space, it inherently embodies the challenge of portability.

\subsection{Non Goals}
Although our goal is to create a fully portable and highly performant scan implementation, we are constrained by underlying hardwares and programming models. As discussed in more detail in \emph{Limits on the Speed-of-Light}, not all architectures offer atomic operations that are sufficiently fast enough or scheduling models that are sufficeintly fair enough to attain speed-of-light performance, and thus we cannot guarantee such performance. Although we are cognisant of the risks posed by subgroup divergence, subgroup functions in shading languages do not allow explicit divergence control\footnote{For example, CUDA allows developers to explicitly provision subgroup functions with a mask of the threads that will participate in it.}, placing a solution to subgroup divergence issues outside our scope. Furthermore, although real-world scans may require more than 30 bits of space for their reductions, or may involve scanning across multiple struct members, WGSL’s current limitations---lack of fences and 64-bit atomics---lead us to focus on the simplest scenario that fits within the 30-bit range. Lastly, we do not investigate alternative $O(2n)$ scan architectures beyond \emph{Chained-Scan}.

\subsection{Achieving Goals}

\section{Implementation}

\subsection{Decoupled-Fallback}

\begin{algorithm}[htbp]
  \SetAlgoLined
  \KwIn{Input data $X$, threshold $\epsilon$}
  \KwOut{Processed data $Y$}
  Initialize $Y \leftarrow \emptyset$\;
  \ForEach{$x \in X$}{
    \If{$x > \epsilon$}{
      Add $x$ to $Y$\;
    }
  }
  \Return{$Y$}\;
  \caption{Decoupled Lookback with Decoupled Fallback}
  \label{alg:example}
\end{algorithm}

\subsection{Intra-Workgroup Implementation}
Our intra-workgroup implementation is most similar to the \emph{CUDPP} scan kernel \cite{}, with adaptations to enhance portability. It consists of three phases:
\begin{enumerate}
  \item
\end{enumerate}

Similar to 3DMS, we split the work tile into non-overlapping blocks, which are distributed to each subgroup. However, unlike CUDA, WGSL lacks a \emph{volatile} qualifier for shared memory, which prevents us from loading items into shared memory and transposing them without inserting a barrier. Placing a barrier disrupts latency hiding by separating the high-latency global memory load from the subsequent transpose and scan computation. To address this, we load data directly into registers and \emph{vectorize} the input. This approach preserves a \emph{coalesced} but vector-strided loading pattern. For each vector, a thread performs a serial scan across the vector, then participates in a subgroup scan. Although this method results in vectors per thread $v$ subgroup scans, it elides the barrier and significantly reduces the kernel's shared memory footprint, as only the subgroup reduction is placed into shared memory. Transposing in shared memory requires a work-tile-sized shared memory allocation, whereas our approach relies more heavily on registers, which are generally more abundant than shared memory, especially on low-spec hardware. Additionally, keeping scan elements in registers avoids shared-bank conflicts, further enhancing performance. As a result, we consider this tradeoff worthwhile.

The WebGPU specification supports subgroup sizes $s$, where $s = 2^k$ and $k \in [2, 7]$, and on some hardware, subgroup sizes can vary between kernel launches. To handle this variability, we use a generalized radix-$s$ Ladner-Fischer scan network embedded with Kogge-Stone subgroup scans to perform the scan on subgroup reductions. The upsweep phase of the Ladner-Fischer network follows a Brent-Kung construction; assuming shared memory bank widths equal to the subgroup size, any subgroup scans beyond the first will experience maximum $s$-way bank conflicts. To mitigate this, we employ Merrill-Grimshaw style conflict avoidance~\cite{}. This network offers several advantages: minimal depth of $\log_2 n$; asymptotically optimal $O(n)$ size; and exposure to bank conflicts is limited to cases where $s^2 < w$, as the network collapses to a single level when $s^2 \geq w$.

\begin{algorithm}[htbp]
  \SetAlgoLined
  \KwIn{Array to scan $x$, Workgroup size $W$, Subgroup size $S$}
  \KwOut{Inclusive scanned array $x$}

  $spine\_length \gets W / S$\;
  $alignment \gets 1 << \text{divRoundUp}(\log_2(spine\_length), \log_2(S)) * \log_2(S)$\;

  $iteration\_offset \gets 0$\;
  $stride \gets 1$\;

  \ForEach{$thread\_id$ \textbf{in} $W$ \textbf{in parallel}}{
    \For{$j \gets S$ \KwTo $alignment$ \textbf{with} $j \gets j * S$}{

      $spine\_index \gets ((thread\_id + iteration\_offset) * stride) - iteration\_offset$\;

      \If{$spine\_index < spine\_length$}{
        $x[spine\_index] \gets \text{subgroupInclusiveScan}(x[spine\_index])$\;
      }

      \textbf{barrier()}\;

      \eIf{$j \neq S$}{
        $reduced\_stride \gets j / stride$\;
        $fanout\_index \gets thread\_id + reduced\_stride$\;

        $cond1 \gets fanout\_index < spine\_length$\;
        $cond2 \gets (fanout\_index \& (j - 1)) \geq reduced\_stride$\;
        $cond3 \gets ((fanout\_index + 1) \& (reduced\_stride - 1)) \neq 0$\;

        \If{$cond1 \textbf{ \&\& } cond2 \textbf{ \&\& } cond3$}{
          $x[fanout\_index] \gets x[fanout\_index] + x[((fanout\_index / stride) * stride) - 1]$\;
        }
      }{
        $iteration\_offset \gets iteration\_offset + 1$\;
      }
      $stride \gets stride * S$\;
    }
  }
  \Return{$x$}\;
  \caption{Subgroup-Size-Agnostic Scan (SAMPLE NO BANK CONFLICT AVOIDANCE)}
  \label{alg:example}
\end{algorithm}

\subsubsection{Size (Work) Efficiency}
For a subgroup of size $s$, the Kogge-Stone scan has a size complexity of $s \log_2 s$. Given an input of size $n$, the work complexity is:
\begin{equation}
  \begin{aligned}[b]
    \text{Work} & = \underbrace{\vphantom{\sum_{k=0}^{\lceil \log_s n \rceil}}n}_{\text{fanout}}
    +
    \underbrace{\sum_{k=1}^{\lceil \log_s n \rceil}}_{\text{iterations}}
    \underbrace{\vphantom{\sum_{k=1}^{\lceil \log_s n \rceil}}\frac{n}{s^k}}_{\text{calls per iteration}}
    \cdot
    \underbrace{\vphantom{\sum_{k=1}^{\lceil \log_s n \rceil}}s \log_2 s}_{\text{work per call}} \\
                & = n + s \log_2 s \cdot \sum_{k=1}^{\lceil \log_s n \rceil} \frac{n}{s^k}       \\
                & = O\left(n + n \log_2 s\right)                                                 \\
                & = O(n \log_2 s).
  \end{aligned}
\end{equation}

\subsubsection{Depth}
The Kogge-Stone scan has a depth of $\log_2 s$ Thus, the total depth of the scan is:
\begin{equation}
  \begin{aligned}[b]
    \text{Depth} & = \underbrace{\lceil \log_s n \rceil}_{\text{loop iterations}}
    \cdot \underbrace{\log_2 s}_{\text{depth per iteration}}                           \\
                 & = \left\lceil \frac{\log_2 n}{\log_2 s} \right\rceil \cdot \log_2 s \\
                 & = \log_2 n + O(1).
  \end{aligned}
\end{equation}
While minimal depth is preserved regardless of the subgroup size, this result can be somewhat misleading, as the depth incurred during the subgroup portion of the scan requires no barrier, whereas the main loop incurs a workgroup-wide barrier at each iteration. Consequently, while the theoretical depth remains constant across subgroup sizes, smaller subgroups tend to perform worse in practice due to the increased relative cost of synchronization and the overhead of additional iterations in the main loop.
\subsection{Full Algorithm}

\section{Experimental Setup}

\section{Results and Analysis}

\section{Discussion}

\subsection{Tradeoffs}

\subsection{Limits on Speed-of-Light Performance}
There are a number of factors which may preclude speed-of-light performance, but most salient is the fairness of the scheduling model. In \emph{Decoupled-Fallback}, a work tile which is late due to unfairness is indistinguishable from one which is deadlocking, so as a scheduler becomes increasingly unfair, it incurs an increasing number of redundant fallbacks. Consider a hardware with a workgroup occupancy denoted by $o$, and let $f$ represent the probability that a fallback operation occurs at a particular lookback step. Because the number of lookback steps is bounded by $o$, the expected number of fallback operations is approximately $fo$. This results in a factor of $fo$ increase in global memory reads and a factor of $fo\log{n}$ increase in work.

This increased sensitivity to fairness exacerabtes existing issues which negatively impact the performance of previous scan architectures, namely lack of compute, and slow atomic update latency. On hardware that lacks sufficient compute power to be memory-bandwidth bound, the additional work incurred by redundant fallback reductions is particularly deleterious. High atomic update latency further compounds the problem: as inter-workgroup communication time grows, the minimum delay before updates become visible to dependent workgroups also increases. This, in turn, raises the number of lookback steps that may be needed and potentially leads to redundant fallbacks.

Fairness has been a priority in NVIDIA hardware since at least the Tesla architecture~\cite{}, but the extent to which it is implemented and prioritized by other vendors remains unclear. While our results on the least fair hardware demonstrate that \emph{Decoupled-Fallback} remains performant, this performance may not extend to scans involving more complex binary reduction operators or less capable hardware.
\begin{acks}
\end{acks}

\bibliographystyle{ACM-Reference-Format}
\bibliography{bib}

\appendix

\section{Research Methods}

\end{document}
\endinput
