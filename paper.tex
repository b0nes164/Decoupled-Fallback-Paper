%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,sigconf')
%%
%% IMPORTANT NOTICE:
%%
%% For the copyright see the source file.
%%
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%%
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%%
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro~\cite [option:text,text]
%TC:macro~\citep [option:text,text]
%TC:macro~\citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.

\documentclass[sigconf]{acmart}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

\setcopyright{acmlicensed}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{XXXXXXX.XXXXXXX}
\acmConference[Conference acronym 'XX]{Make sure to enter the correct
  conference title from your rights confirmation emai}{June 03--05,
  2018}{Woodstock, NY}
\acmISBN{978-1-4503-XXXX-X/18/06}

\usepackage[ruled,vlined]{algorithm2e}
\usepackage{amsmath}
\usepackage{mathtools}
\newcommand{\thomas}[1]{{\footnotesize\color{orange}[Thomas: #1]}}
\newcommand{\john}[1]{{\footnotesize\color{cyan}[John: #1]}}
\newcommand{\raph}[1]{{\footnotesize\color{magenta}[Raph: #1]}}

\begin{document}

\title{The Name of the Title Is Hope}

\author{Lars Th{\o}rv{\"a}ld}
\affiliation{%
\institution{The Th{\o}rv{\"a}ld Group}
\city{Hekla}
\country{Iceland}}
\email{larst@affiliation.org}

\author{Valerie B\'eranger}
\affiliation{%
  \institution{Inria Paris-Rocquencourt}
  \city{Rocquencourt}
  \country{France}
}

\author{Aparna Patel}
\affiliation{%
  \institution{Rajiv Gandhi University}
  \city{Doimukh}
  \state{Arunachal Pradesh}
  \country{India}}

\renewcommand{\shortauthors}{Trovato et al.}

\begin{abstract}
  TODO
\end{abstract}

\begin{CCSXML}
  <ccs2012>
  <concept>
  <concept_id>00000000.0000000.0000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>500</concept_significance>
  </concept>
  <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>300</concept_significance>
  </concept>
  <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>100</concept_significance>
  </concept>
  <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>100</concept_significance>
  </concept>
  </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
\ccsdesc[300]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
\ccsdesc{Do Not Use This Code~Generate the Correct Terms for Your Paper}
\ccsdesc[100]{Do Not Use This Code~Generate the Correct Terms for Your Paper}

\keywords{Do, Not, Us, This, Code, Put, the, Correct, Terms, for,
  Your, Paper}

\received{20 February 2007}
\received[revised]{12 March 2009}
\received[accepted]{5 June 2009}

\maketitle

\section{Introduction}
(Tell readers we are targetting WebGPU)
(Speed of light definition here)
\subsection{Goals of a Scan Implementation}
\begin{itemize}
  \item \textbf{Minimal global memory access:} the scan should minimize global memory access to the theoretical minimum of $2n$.
  \item \textbf{Fully saturates global memory bandwidth:} the scan should be capable of fully saturating the global memory bandwidth.
  \item \textbf{Portability:} the scan should be portable across hardware vendors and architectures.
\end{itemize}

\section{Background}
\subsection{The GPU Programming Environment(Model?)}
\john{So I have written, roughly, a zillion of these sections and here is my current philosophy. The danger is that you write something really interesting and relevant and a reviewer who is familiar with GPUs sees the title and sees the first few lines and says ``I can skip this'' and does. There are plenty of great descriptions of how GPUs work. Cite one of them. What this section needs to do is very specifically call out the interesting/detailed features of GPUs that are \emph{particularly relevant to this paper} that are not just ``here's generally how a GPU works''. For me, that's the last paragraph. The other part that might be necessary is defining the specific terminology used in the paper (e.g., workgroup vs.\ thread block vs.\ CTA). If you feel like breaking up a wall of text, which I generally think is a good idea, we can make a little table of equivalent terminology and boldface the terms we're using.}
Contemporary GPUs are hierarchically organized, massively parallel processors designed to prioritize throughput over latency. Due to their specialized nature, GPUs function as co-processors to the CPU, requiring GPU programs—known as \emph{kernels} or \emph{shaders}—to be \emph{launched} or \emph{dispatched} heterogeneously by the CPU\@. Upon launch, a kernel is replicated across independent execution contexts, called threads. These threads are first organized into smaller units known as \emph{subgroups}\footnote{Also referred to as \emph{Warps}, \emph{Waves}, or \emph{SIMD-Groups}.}, which are then grouped into larger units called \emph{workgroups}\footnote{Also referred to as \emph{Cross-Thread-Arrays}, \emph{Thread-Blocks}, \emph{Blocks}, \emph{Thread-Groups}, or \emph{Groups}.}. This thread hierarchy mirrors the GPU memory hierarchy: individual threads access private, high-speed registers; threads within a workgroup share low-latency shared memory\footnote{Also referred to as \emph{Local Data Storage}, \emph{Group-Shared Memory}, (Check metal terminology).}; and all threads have access to high-bandwidth but higher-latency global memory.

While threads within the same workgroup can synchronize with each other using barrier primitives and can communicate through shared memory, a workgroup is a logical unit of execution, not a physical processor. Instead, workgroups are dynamically mapped and scheduled to a GPU's underlying \emph{multiprocessors}. This abstraction enables the kernel to execute on GPUs with varying hardware capabilities, such as different numbers of multiprocessors or multiprocessors with differing execution resources. A single multiprocessor can host multiple workgroups; we define the number of active subgroups on a multiprocessor as the \emph{subgroup occupancy}, and the number of active workgroups across the entire GPU as the \emph{workgroup occupancy}. Generally speaking, high subgroup occupancy is desirable because multiprocessors are specifically designed to context switch between subgroups, keeping execution units busy and masking memory and execution latency.

GPUs exhibit both single-instruction, multiple-thread (SIMT) and single-program, multiple-data (SPMD) behaviors. A multiprocessor schedules and executes threads by subgroup. While it attempts to execute the same instruction across all threads in the subgroup, SIMT, threads within the same subgroup are free to follow their own data/context-dependent execution paths, SPMD\@. However, this flexibility comes at the cost of subgroup divergence. When threads in a subgroup diverge due to conditional branching or other control flow, each unique path is executed sequentially, but at the full execution width, effectively rendering the operation serial and reducing parallel efficiency.\footnote{The scheduling and execution width of the underlying hardware does not necessarily align with the subgroup width.}

Most relevant to this work is the scheduling behavior of the GPU\@. Scheduling is divided into a workgroup-level scheduler\footnote{For example, NVIDIA's \emph{GigaThread Engine} or AMD's \emph{Asynchronous Compute Engines}}, which manages kernel launches and maps workgroups to multiprocessors, and the aforementioned subgroup scheduler, responsible for selecting which of the currently resident subgroups for execution. Although contemporary GPU hardware supports workgroup preemption to prioritize real-time tasks or manage multi-process workloads~\cite{}(NVIDIA Volta,AMD RDNA), this capability does not extend to the GPU programming model. Once a workgroup begins execution on a multiprocessor, it must run to completion and cannot context switch. At the subgroup scheduling level, \emph{fairness}—how evenly execution resources are distributed among threads—and \emph{progress guarantees}—ensuring that all threads or subgroups eventually make forward progress—are the most salient issues. As we will discuss in the following section, these factors have profound implications for kernel performance and correctness.

\subsection{The Scan Primitive}
The study of \emph{scan} (\emph{parallel prefix}) networks traces back to the design of carry-lookahead adder circuits and beyond~\cite{10.1145/322217.322232, 5219801}. A scan is typically defined on a monoid \( M \), characterized by a binary reduction operator \( \oplus \) and an identity element \( \varepsilon \). The binary operator \( \oplus \) satisfies the closure property \( \forall a, b \in M, \ (a \oplus b) \in M \) and has an identity element \( \exists \varepsilon \in M, \ \forall a \in M, \ \varepsilon \oplus a = a \). Although \( \oplus \) must be associative, it is not necessarily commutative, as demonstrated in structures like the stack monoid~\cite{}. \john{Rabin-Karp string matching (``Efficient randomized pattern-matching algorithms'', Section 6, 1987)~\cite[Section 6]{Karp:1987:ERP} has 2$\times$2 matrix multiplication as its binary operation, and is a good thing to cite in a paper going to SPAA\@.} In a scan, the result at the \( n \)-th element is the reduction of the preceding subset of elements in the sequence. If the subset includes the \( n \)-th element, it is called \textit{inclusive}; if it excludes the \( n \)-th element, it is called \textit{exclusive}. The most common scan type is the prefix sum, where \( \oplus \) is addition. For example:
\[
  x = [x_1, x_2, x_3, \dots, x_n] \ \ \ \ \ \ \ y = [1, 1, 1, 1, 1]
\]
\[
  \text{InclusiveScan}(x, \oplus) = [x_1, x_1 \oplus x_2, x_1 \oplus x_2 \oplus x_3, \dots, x_1 \oplus x_2 \oplus \cdots \oplus x_n]
\]
\[
  \text{InclusiveScan}(y, +) = [1, 2, 3, 4, 5]
\]
Due to its significance in circuits and as a fundamental algorithmic primitive, scan has been extensively studied in both electrical engineering and computer science. Harris offers a taxonomy that relates depth, fanout, and wire tracks~\cite{}, while Hinze develops an algebraic framework for scans~\cite{}. Snir proved that \emph{depth} and \emph{size} are related by (fill in)~\cite{}, and Fich proved that among minimum depth scans Ladner-Fischer networks have optimal size. Finally, Merrill and Garland provide a comprehensive review of GPU scan implementations. In this work, we focus on the evolution of inter-workgroup (global) scan strategies and evaluate their suitability for our ideal architecture.

\subsection{Evolution of Inter-Workgroup Scan Architectures}
Prior to the introduction of shared memory, early GPU scan implementations~\cite{Horn, Hensley, Greß, Sengupta} operated directly on global memory and performed a single global scan where each level of depth necessitated a separate kernel launch. The introduction of shared memory and dedicated compute environments resulted in the first implementations to resemble contemporary designs, combining coarse-grained inter-workgroup strategies with fine-grained intra-workgroup strategies. \emph{Work tile} partitioning was introduced to divide inputs into workgroup-consumable chunks that fit within the limited size of shared memory. However, as a natural result of this partitioning, a serial dependency is created between the work tiles. In this section, we review the three prominent global scan strategies—\emph{Scan-then-Propagate}, \emph{Reduce-then-Scan}, and \emph{Chained-Scan}—examining how they navigate this inter-workgroup dependency and evaluating their effectiveness in minimizing global memory access and maximizing memory bandwidth utilization in alignment with our ideal architecture goals.

\subsubsection{Scan-then-Propagate}
Introduced by Sengupta et al.~\cite{10.5555/1280094.1280110}, the \emph{Scan-then-Propagate}~\cite{GPUGems3, Sengupta2008} strategy is an intuitive extension of the intra-workgroup scan, comprising three phases:
\begin{enumerate}
  \item \textbf{Intra-Workgroup (Local) Scan:} Each workgroup performs an inclusive scan on its assigned work-tile partition of the input data. The results, including both the local scan output and the reduction of each workgroup, are written back to global memory.
  \item \textbf{Spine Scan:} The reductions from all workgroups, collectively referred to as \emph{the spine}, are gathered and scanned to compute the global prefix of the reductions, resolving the dependencies between workgroups; we call the result of this phase the \emph{root}.
  \item \textbf{Propagation:} The root is propagated back to each workgroup and combined with the local scan results to compute the final scan.
\end{enumerate}
Without an inter-workgroup barrier primitive, each phase is implemented as a separate kernel launch to ensure a consistent view of the reductions. During the scan operation, each element is read from and written to global memory once during the local scan and again during the propagation phase, contributing a total of $4n$ global memory operations. Although data movement is dominated by the first and third phase, Sengupta et al.'s implementation handles arbitrarily large inputs using recursion. For an input of size $n$ and a work tile of size $t$, this results in a recursive depth of $\lceil \log_t n \rceil$. At each recursive step $k$, $n/t^k$ tiles are processed, leading to a total of $\sum_{k=1}^{\lceil \log_t n \rceil} n/t^k \approx (n - 1)/(t - 1)$ work tiles over all steps. Since each tile moves $4t$ data ($2t$ local scan, $2t$ propagation), the total data movement is $O\left(\frac{4t(n - 1)}{t - 1}\right) = O(4n)$.

\subsubsection{Reduce-then-Scan}
Due to the extreme latency of global memory access relative to compute, computationally light algorithms like \emph{scan} can benefit from redundant calculation if it reduces memory access. This insight led to a refinement of the \emph{Scan-Then-Propagate} strategy called \emph{Reduce-Then-Scan}~\cite{Merrill-Grimshaw, Ha-and-Han, Dotsenko, Breitbart}: by writing only the work tile reduction in the first phase and performing a redundant scan in the third phase, $n$ global data movement can be eliminated. The \emph{Reduce-Then-Scan} strategy operates in three phases:
\begin{enumerate}
  \item \textbf{Work Tile Reduction:} Each workgroup performs a pure reduction on its work tile partition, posting the result into global memory.
  \item \textbf{Spine Scan:} The reductions from all workgroups are gathered and scanned to produce the root.
  \item \textbf{Intra-Workgroup Scan and Propagation:} Each workgroup performs the local scan on the work tile, incorporating the root as it writes to global memory.
\end{enumerate}
Merrill and Grimshaw~\cite{} introduced a workgroup-level \emph{raking} technique that improved the spine phase of \emph{Reduce-Then-Scan}. Instead of handling arbitrary inputs recursively, this approach determines the total workgroup occupancy $o$ and partitions the input into non-overlapping blocks, processed serially by each workgroup. By limiting the recursive depth of the global scan to two, the global memory movement for the spine scan is reduced from $O(n/t)$ to $O(3o)$. Consequently, kernel launches are limited to three, and total global memory movement is reduced to $O(3n + 3o) = O(3n)$. This approach was further refined by Ha and Han~\cite{}, who modified the upsweep behavior of the first workgroup from reduction to scan and eliminated the redundant upsweep of the last workgroup, thereby saving the equivalent of one workgroup's computation.

\subsubsection{Chained-Scan}
Continuing the trend of global data movement optimization, the most recent work on global scan has been the \emph{Chained-Scan} architecture. First introduced by Yan et al.\ in \emph{StreamScan}, the key innovation of GPU chained scanning lies in its hybridization of parallel and serial strategies: achieving parallelism at the intra-workgroup level while minimizing global data movement through serial scan operations at the inter-workgroup level. Instead of using kernel launches as inter-workgroup synchronization points, \emph{Chained-Scans} launch a single kernel that assigns work tiles in a serial order to workgroups. This shifts the problem from explicit synchronization to implicit synchronization via the order in which workgroups access global data. Each workgroup processes its assigned tile independently, waiting only for the result of the preceding reductions before continuing. This structure ensures that global memory traffic is minimized, as each element is read and written exactly once, achieving the theoretical minimum of $2n$.

While \emph{StreamScan} successfully reduces per-element device memory accesses to two, it does not consistently saturate global memory bandwidth, preventing it from achieving speed-of-light performance. This limitation arises because a workgroup in \emph{StreamScan} cannot post its reduction to global memory until it has received the reduction of the preceding workgroup. Although this reduces global memory access during the propagation phase to $O(n/t)$, the sequential dependency creates a bottleneck, constraining the algorithm’s performance to the rate at which reductions propagate through device memory. Despite this limitation, the total data movement remains optimal at $O(2n + n/t) = O(2n)$.

Building on \emph{StreamScan}, Merrill and Garland introduced the \emph{Decoupled Lookback} technique to address the bottleneck created by sequential dependencies. Unlike \emph{StreamScan}, where reductions and dependency resolution occur sequentially, \emph{Decoupled Lookback} separates these processes. In this approach, each workgroup posts its local reduction to global memory and then performs a \emph{lookback}—a serial traversal backward along the spine to resolve dependencies. The work performed during this phase is constant-bounded to the workgroup occupancy $o$, as redundant operations are avoided by updating the workgroup's reduction from local to inclusive once the lookback is completed. Although this increases global memory access during the propagation phase to $O(o(n/t))$, the serial ordering of work tile processing ensures that memory accesses to the spine are roughly contiguous within the workgroup occupancy block, making them highly likely to be cache-resident. As a result, the additional global memory traffic along the spine introduces a negligible latency overhead. With a total global memory movement of $O(2n+o(n/t))= O(2n)$, \emph{Decoupled Lookback} achieves full utilization of device memory bandwidth and delivers speed-of-light performance.

\subsection{Ideal Scan Architecture}

In addition to its speed, the \emph{Chain-Scan} architecture has other, more practical properties, that improve over its predecessors. As it is a single kernel, it minimizes launch overheads and simplifies maintenance. On previous architectures, handling of arbitrary inputs can be awkward: the recursive approach, for example, entails multiple kernel launches and a heterogenous system to coordinate them. Similarly, a raking approach can be cumbersome, as determining the workgroup occupancy for every target device is often impractical. Furthermore, on GPUs where the global memory data cache is indexed by physical addresses—and therefore requires lookups in \emph{translation lookaside buffers} (TLBs)—raking the workgroups can lead to performance degradation due to cache thrashing. This issue occurs when task sizes are large enough that each workgroup accesses its own unique page entry, and the device lacks sufficient TLB coverage to support the memory access patterns of all workgroups. As a consequence of its $2n$ global data movement and inherently serial ordering, \emph{Chained-Scan} is also suitable for \emph{in-situ} compaction and allocation. In previous architectures, such tasks often required additional kernel launches or complex bookkeeping to handle dependencies between workgroups. By contrast, \emph{Chained-Scan} efficiently propagates dependencies within a single kernel, minimizing overhead and enabling seamless integration of compaction and allocation operations.

Thus, while \emph{Chained-Scan} with \emph{Decoupled Lookback} appears to be the ideal scan architecture, it falls short in meeting the criterion of \emph{portability}. As we discuss in more detail in the following section, this limitation arises because \emph{Chained-Scan} depends on a \emph{forward-progress guarantee}, a feature provided by NVIDIA hardware but not universally available across all vendors and architectures. Without this guarantee, \emph{Chained-Scan} is highly prone to catastrophic failure, making it unsuitable for the inherently cross-vendor, cross-architecture environment of shading languages.

\subsection{Why does \emph{Chained-Scan} Rely on Forward-Progress Guarantees?}
A \emph{forward-progress guarantee} (FPG) is a promise provided by the subgroup scheduler that all occupant subgroups eventually make progress toward termination, preventing scenarios where a subgroup is starved of execution time. Sorensen et al.~\cite{} and Sorensen et al.~\cite{} provide the most comprehensive analysis of GPU schedulers, including the implications of running FPG dependent algorithms without it.

In \emph{Chained-Scan}, workgroups operate in a producer-consumer framework: each workgroup produces the reduction of its assigned work tile, while subsequent workgroups consume the reductions of all preceding workgroups to complete the scan. To enforce this serial ordering, work tiles are dynamically assigned using atomic increment operations, rather than static assignment based on virtualized workgroup indices. While this dynamic assignment guarantees that predecessor work tiles are either scheduled or completed, without FPG there is no guarantee that these reductions will eventually become available. A scheduler lacking FPG may fail to allocate sufficient execution time to certain workgroups, leaving their reductions incomplete indefinitely. In such cases, consuming workgroups stall during their lookback phase, waiting for reductions that are never posted to global memory.

For instance, consider a scan operation requiring two workgroups running on a GPU with a single multiprocessor. Although the multiprocessor can host both workgroups, it has only one scheduling unit, which lacks FPG\@. In this setup, both workgroups launch and acquire their respective work tiles. The workgroup responsible for \emph{Work Tile 1} finishes first and begins waiting for the reduction of \emph{Work Tile 0}. However, because the scheduler lacks FPG, it may continue scheduling \emph{Work Tile 1}, leaving \emph{Work Tile 0} starved and unable to complete its reduction. As a result, \emph{Work Tile 1} spins indefinitely, eventually triggering \emph{timeout detection and recovery} (TDR) and crashing the host program.

\subsection{Why is reliance on Forward Progress Guarantees a Portability Problem?}
Although NVIDIA formalized FPG down to the intra-subgroup level beginning with the Volta architecture---and FPG was likely already present at the workgroup level on at least NVIDIA's Fermi and AMD's TeraScale2 architectures~\cite{}---it remains absent on some contemporary hardware, most notably certain ARM-based chips, including Apple's M-series GPUs~\cite{10.1145/3485508}. Our testing shows that, at best, attempting to run \emph{Chained-Scan} without FPG results in mega-bad\footnote{Need to gather data on the distribution of M1 Pro time on Decoupled Lookback instead of the average time. It's also unclear why the test did not trigger Metal's timeout device recovery}, and at worst, risks the afformentioned TDR and subsequent program crash. This extreme slowdown highlights the inherent fragility of \emph{Chained-Scan} without FPG\@.

Exacerbating this issue is the fact that no major graphics API currently provides developers with a mechanism to query FPG support\footnote{As of writing this, 12/29/2024, we are unaware of any major graphics API---Vulkan, D3D12, or Metal---that provide developers with an FPG capability query}. This lack of transparency forces developers to design algorithms without clear knowledge of whether critical guarantees are available on the target hardware. In environments like shading languages, where cross-platform portability is required, this uncertainty compels developers to revert to older, less efficient strategies such as \emph{Reduce-Then-Scan}. However, as discussed earlier, \emph{Reduce-Then-Scan} carries its own set of drawbacks, including increased global memory traffic ($O(3n)$) and additional kernel launches, which result in reduced throughput and higher latency. While these tradeoffs were once deemed acceptable, they stand in stark contrast to the efficiency and simplicity of \emph{Chained-Scan} when implemented on hardware with reliable FPG\@. Yet, in the absence of guaranteed forward progress, developers have little choice but to sacrifice performance for broader compatibility.

\subsection{Earlier Attempts at Portability (Cuttable?)}
Levien and Naur~\cite{}, made the first attempt at solving forward progress in \emph{DecoupledLookback}, introducing \emph{ScallarFallback}. In \emph{ScallarFallback}, each time a workgroup waits on a preceding tile, it begins a \emph{fallback} operation, and begins consuming the blocking tile one element at a time. Although this entails performing redundant work when the blocking tile is simply late but not deadlocking, it guarantees the termination of the algorithm because a workgroup can also perform the reduction of a blocking tile itself, thus breaking the inter-workgroup dependency.

\emph{ScallarFallback} suffers from two primary issues that impact its efficiency. First, the fallback reduction is performed serially by a single thread, progressing only when the workgroup engages in a waiting spin.This effectively takes $t$ spins to complete the fallback of the deadlocking tile. Moreover, because only a single thread is responsible for loading elements, the cache line is severely underutilized, further degrading performance. Second, once the fallback is complete, the workgroup does not post its result to device memory. Consequently, every workgroup deadlocked by that tile must redundantly execute the fallback, wasting computational resources. While \emph{ScallarFallback} was able to achieve speed of light on hardware supporting FPG and executed correctly on hardware without FPG, its performance on the latter was inferior to that of \emph{Reduce-Then-Scan}.

\section{Design}
\subsection{Goals}
This work is guided by two main goals. First, portability: we want to develop a scan implementation that retains the benefits of the \emph{Chained-Scan} architecture but is also suitable for a diverse range of hardware vendors and achitectures, including those without FPG\@. Second, performance: we aim for near speed-of-light execution to the greatest extent possible allowed by the underlying hardware and programming model. To ground these objectives, we select the WebGPU shading language (WGSL) and the WebGPU standard as our target environment. WGSL is a meta-level shading language which is translated and compiled as necessary for backend graphics APIs---D3D12, Vulkan, and Metal---and as such, WGSL implementations are bound by the minimum capability across all backends. Because WGSL must operate within this limited capability space, it inherently embodies the challenge of portability.

\subsection{Non Goals}
Although our goal is to create a fully portable and highly performant scan implementation, we are constrained by underlying hardwares and programming models. As discussed in more detail in \emph{Limits on the Speed-of-Light}, not all architectures offer atomic operations that are sufficiently fast enough or scheduling models that are sufficeintly fair enough to attain speed-of-light performance, and thus we cannot guarantee such performance. Although we are cognisant of the risks posed by subgroup divergence, subgroup functions in shading languages do not allow explicit divergence control\footnote{For example, CUDA allows developers to explicitly provision subgroup functions with a mask of the threads that will participate in it.}, placing a solution to subgroup divergence issues outside our scope. Furthermore, although real-world scans may require more than 30 bits of space for their reductions, or may involve scanning across multiple struct members, WGSL’s current limitations---lack of fences and 64-bit atomics---lead us to focus on the simplest scenario that fits within the 30-bit range. Lastly, we do not investigate alternative $O(2n)$ scan architectures beyond \emph{Chained-Scan}.

\section{Implementation}

\subsection{Decoupled-Fallback}

\begin{algorithm}[htbp]
  \SetAlgoLined
  \KwIn{Input data $X$, threshold $\epsilon$}
  \KwOut{Processed data $Y$}
  Initialize $Y \leftarrow \emptyset$\;
  \ForEach{$x \in X$}{
    \If{$x > \epsilon$}{
      Add $x$ to $Y$\;
    }
  }
  \Return{$Y$}\;
  \caption{Decoupled Lookback with Decoupled Fallback}
  \label{alg:example}
\end{algorithm}

\subsection{Intra-Workgroup Implementation}
Our intra-workgroup implementation is most similar to the \emph{CUDPP} scan kernel~\cite{}, with adaptations to enhance portability. It consists of three phases:
\begin{enumerate}
  \item
\end{enumerate}

Similar to 3DMS, we split the work tile into non-overlapping blocks, which are distributed to each subgroup. However, unlike CUDA, WGSL lacks a \emph{volatile} qualifier for shared memory, which prevents us from loading items into shared memory and transposing them without inserting a barrier. Placing a barrier disrupts latency hiding by separating the high-latency global memory load from the subsequent transpose and scan computation. To address this, we load data directly into registers and \emph{vectorize} the input. This approach preserves a \emph{coalesced} but vector-strided loading pattern. For each vector, a thread performs a serial scan across the vector, then participates in a subgroup scan. Although this method results in vectors per thread $v$ subgroup scans, it elides the barrier and significantly reduces the kernel's shared memory footprint, as only the subgroup reduction is placed into shared memory. Transposing in shared memory requires a work-tile-sized shared memory allocation, whereas our approach relies more heavily on registers, which are generally more abundant than shared memory, especially on low-spec hardware. Additionally, keeping scan elements in registers avoids shared-bank conflicts, further enhancing performance. As a result, we consider this tradeoff worthwhile.

The WebGPU specification supports subgroup sizes $s$, where $s = 2^k$ and $k \in [2, 7]$, and on some hardware, subgroup sizes can vary between kernel launches. To handle this variability, we use a generalized radix-$s$ Ladner-Fischer scan network embedded with Kogge-Stone subgroup scans to perform the scan on subgroup reductions. The upsweep phase of the Ladner-Fischer network follows a Brent-Kung construction; assuming shared memory bank widths equal to the subgroup size, any subgroup scans beyond the first will experience maximum $s$-way bank conflicts. To mitigate this, we employ Merrill-Grimshaw style conflict avoidance~\cite{}. This network offers several advantages: minimal depth of $\log_2 n$; asymptotically optimal $O(n)$ size; and exposure to bank conflicts is limited to cases where $s^2 < w$, as the network collapses to a single level when $s^2 \geq w$.

\begin{algorithm}[htbp]
  \SetAlgoLined
  \KwIn{Array to scan $x$, Workgroup size $W$, Subgroup size $S$}
  \KwOut{Inclusive scanned array $x$}

  $spine\_length \gets W / S$\;
  $alignment \gets 1 << \text{divRoundUp}(\log_2(spine\_length), \log_2(S)) * \log_2(S)$\;

  $iteration\_offset \gets 0$\;
  $stride \gets 1$\;

  \ForEach{$thread\_id$ \textbf{in} $W$ \textbf{in parallel}}{
    \For{$j \gets S$ \KwTo $alignment$ \textbf{with} $j \gets j * S$}{

      $spine\_index \gets ((thread\_id + iteration\_offset) * stride) - iteration\_offset$\;

      \If{$spine\_index < spine\_length$}{
        $x[spine\_index] \gets \text{subgroupInclusiveScan}(x[spine\_index])$\;
      }

      \textbf{barrier()}\;

      \eIf{$j \neq S$}{
        $reduced\_stride \gets j / stride$\;
        $fanout\_index \gets thread\_id + reduced\_stride$\;

        $cond1 \gets fanout\_index < spine\_length$\;
        $cond2 \gets (fanout\_index \& (j - 1)) \geq reduced\_stride$\;
        $cond3 \gets ((fanout\_index + 1) \& (reduced\_stride - 1)) \neq 0$\;

        \If{$cond1 \textbf{ \&\& } cond2 \textbf{ \&\& } cond3$}{
          $x[fanout\_index] \gets x[fanout\_index] + x[((fanout\_index / stride) * stride) - 1]$\;
        }
      }{
        $iteration\_offset \gets iteration\_offset + 1$\;
      }
      $stride \gets stride * S$\;
    }
  }
  \Return{$x$}\;
  \caption{Subgroup-Size-Agnostic Scan (SAMPLE NO BANK CONFLICT AVOIDANCE)}
  \label{alg:example}
\end{algorithm}

\subsubsection{Size (Work) Efficiency}
For a subgroup of size $s$, the Kogge-Stone scan has a size complexity of $s \log_2 s$. Given an input of size $n$, the work complexity is:
\begin{equation}
  \begin{aligned}[b]
    \text{Work} & = \underbrace{\vphantom{\sum_{k=0}^{\lceil \log_s n \rceil}}n}_{\text{fanout}}
    +
    \underbrace{\sum_{k=1}^{\lceil \log_s n \rceil}}_{\text{iterations}}
    \underbrace{\vphantom{\sum_{k=1}^{\lceil \log_s n \rceil}}\frac{n}{s^k}}_{\text{calls per iteration}}
    \cdot
    \underbrace{\vphantom{\sum_{k=1}^{\lceil \log_s n \rceil}}s \log_2 s}_{\text{work per call}} \\
                & = n + s \log_2 s \cdot \sum_{k=1}^{\lceil \log_s n \rceil} \frac{n}{s^k}       \\
                & = O\left(n + n \log_2 s\right)                                                 \\
                & = O(n \log_2 s).
  \end{aligned}
\end{equation}

\subsubsection{Depth}
The Kogge-Stone scan has a depth of $\log_2 s$ Thus, the total depth of the scan is:
\begin{equation}
  \begin{aligned}[b]
    \text{Depth} & = \underbrace{\lceil \log_s n \rceil}_{\text{loop iterations}}
    \cdot \underbrace{\log_2 s}_{\text{depth per iteration}}                           \\
                 & = \left\lceil \frac{\log_2 n}{\log_2 s} \right\rceil \cdot \log_2 s \\
                 & = \log_2 n + O(1).
  \end{aligned}
\end{equation}
While minimal depth is preserved regardless of the subgroup size, this result can be somewhat misleading, as the depth incurred during the subgroup portion of the scan requires no barrier, whereas the main loop incurs a workgroup-wide barrier at each iteration. Consequently, while the theoretical depth remains constant across subgroup sizes, smaller subgroups tend to perform worse in practice due to the increased relative cost of synchronization and the overhead of additional iterations in the main loop.
\subsection{Full Algorithm}

\section{Experimental Setup}

\section{Results and Analysis}

\section{Discussion}

\subsection{Tradeoffs}

\subsection{Limits on Speed-of-Light Performance}
There are a number of factors which may preclude speed-of-light performance, but most salient is the fairness of the scheduling model. In \emph{Decoupled-Fallback}, a work tile which is late due to unfairness is indistinguishable from one which is deadlocking, so as a scheduler becomes increasingly unfair, it incurs an increasing number of redundant fallbacks. Consider a hardware with a workgroup occupancy denoted by $o$, and let $f$ represent the probability that a fallback operation occurs at a particular lookback step. Because the number of lookback steps is bounded by $o$, the expected number of fallback operations is approximately $fo$. This results in a factor of $fo$ increase in global memory reads and a factor of $fo\log{n}$ increase in work.

This increased sensitivity to fairness exacerabtes existing issues which negatively impact the performance of previous scan architectures, namely lack of compute, and slow atomic update latency. On hardware that lacks sufficient compute power to be memory-bandwidth bound, the additional work incurred by redundant fallback reductions is particularly deleterious. High atomic update latency further compounds the problem: as inter-workgroup communication time grows, the minimum delay before updates become visible to dependent workgroups also increases. This, in turn, raises the number of lookback steps that may be needed and potentially leads to redundant fallbacks.

Fairness has been a priority in NVIDIA hardware since at least the Tesla architecture~\cite{}, but the extent to which it is implemented and prioritized by other vendors remains unclear. While our results on the least fair hardware demonstrate that \emph{Decoupled-Fallback} remains performant, this performance may not extend to scans involving more complex binary reduction operators or less capable hardware.
\begin{acks}
\end{acks}

\bibliographystyle{ACM-Reference-Format}
\bibliography{bib}

\appendix

\section{Research Methods}

\end{document}
\endinput

%%% Local Variables:
%%% mode: LaTeX
%%% TeX-master: t
%%% End:
