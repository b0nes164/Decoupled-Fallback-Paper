%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,sigconf')
%%
%% IMPORTANT NOTICE:
%%
%% For the copyright see the source file.
%%
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%%
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%%
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro~\cite [option:text,text]
%TC:macro~\citep [option:text,text]
%TC:macro~\citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.

\documentclass[sigconf]{acmart}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

\setcopyright{acmlicensed}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{XXXXXXX.XXXXXXX}
\acmConference[Conference acronym 'XX]{Make sure to enter the correct
  conference title from your rights confirmation emai}{June 03--05,
  2018}{Woodstock, NY}
\acmISBN{978-1-4503-XXXX-X/18/06}

\usepackage[ruled,vlined]{algorithm2e}
\begin{document}

\title{The Name of the Title Is Hope}

\author{Lars Th{\o}rv{\"a}ld}
\affiliation{%
\institution{The Th{\o}rv{\"a}ld Group}
\city{Hekla}
\country{Iceland}}
\email{larst@affiliation.org}

\author{Valerie B\'eranger}
\affiliation{%
  \institution{Inria Paris-Rocquencourt}
  \city{Rocquencourt}
  \country{France}
}

\author{Aparna Patel}
\affiliation{%
  \institution{Rajiv Gandhi University}
  \city{Doimukh}
  \state{Arunachal Pradesh}
  \country{India}}

\renewcommand{\shortauthors}{Trovato et al.}

\begin{abstract}
  TODO
\end{abstract}

\begin{CCSXML}
  <ccs2012>
  <concept>
  <concept_id>00000000.0000000.0000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>500</concept_significance>
  </concept>
  <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>300</concept_significance>
  </concept>
  <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>100</concept_significance>
  </concept>
  <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>100</concept_significance>
  </concept>
  </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
\ccsdesc[300]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
\ccsdesc{Do Not Use This Code~Generate the Correct Terms for Your Paper}
\ccsdesc[100]{Do Not Use This Code~Generate the Correct Terms for Your Paper}

\keywords{Do, Not, Us, This, Code, Put, the, Correct, Terms, for,
  Your, Paper}

\received{20 February 2007}
\received[revised]{12 March 2009}
\received[accepted]{5 June 2009}

\maketitle

\section{Introduction}
 Do. this. last.
 
\section{Background}
\subsection{The GPU Programming Environment(Model?)}
 Contemporary GPUs are hierarchically organized, massively parallel processors designed to prioritize throughput over latency. GPU programming exhibits both single-instruction, multiple-thread (SIMT) and single-program, multiple-data (SPMD) behaviors. When \emph{launched}\footnote{Also referred to as \emph{dispatched}.}, a GPU program—known as a \emph{kernel}—is replicated across independent execution contexts, referred to as threads. Threads are organized into groups called \emph{workgroups}\footnote{Also referred to as \emph{Cross-Thread-Arrays}, \emph{Thread-Blocks}, \emph{Blocks}, \emph{Thread-Groups}, or \emph{Groups}.}, which are further divided into smaller units known as \emph{subgroups}\footnote{Also referred to as \emph{Warps}, \emph{Waves}, or \emph{SIMD-Groups}.}. This thread hierarchy mirrors the GPU memory hierarchy: individual threads access private, high-speed registers; threads within a workgroup share low-latency shared memory\footnote{Also referred to as \emph{Local Data Storage}, \emph{Group-Shared Memory}, (Check metal terminology).}; and all threads have access to high-bandwidth but higher-latency global memory.

 While threads within the same workgroup share resources, a workgroup is a logical unit of execution, not a physical processor. Instead, a GPU's underlying multiprocessors dynamically map and schedule the workgroups. This abstraction enables the kernel to be executed on GPUs with varying hardware capabilities, such as different numbers of multiprocessors or multiprocessors with differing execution resources. A single multiprocessor can host multiple workgroups; we define the number of active subgroups on a multiprocessor as the \emph{subgroup occupancy}, and the number of active workgroups across the entire GPU as the \emph{workgroup occupancy}. High subgroup occupancy is generally desirable as it hides memory and execution latency by ensuring sufficient warps are available for scheduling while others wait for data.
 
 A multiprocessor schedules and executes threads by subgroup. While it attempts to execute the same instruction across all threads in the subgroup, SIMT, threads within the same subgroup are free to follow their own data/context-dependent execution paths, SPMD. However, this flexibility comes at the cost of subgroup divergence. When threads in a subgroup diverge due to conditional branching or other control flow, each unique path is executed sequentially, but at the full execution width, effectively rendering the operation serial and reducing parallel efficiency.\footnote{The scheduling and execution width of the underlying hardware does not necessarily align with the subgroup width.} 
 
 As we will discuss in the seciton, (insert section here), how a multiprocessor chooses \emph{which} out of the currently occupying subgroups to schedule can have a profound impact on both the performance and the correct execution of kernels. 

\subsection{Evolution of Inter-Workgroup Scan Architectures}
\subsubsection{Scan-then-Propagate}
\subsubsection{Reduce-then-Scan}
\subsubsection{Chained-Scan}

\subsection{Why does \emph{Chained-Scan} Rely on Forward Progress Guarantees?}
 In a scan, the reduction at each element is dependent on the reduction of preceding elements. Thus, a serial dependency is created between workgroups whenever the number of elements in a scan operation exceeds the maximum size that can be processed by a single workgroup. Historically, hardware vendors have only recently begun to formally support inter-workgroup synchronization. Prior to the introduction of \emph{thread block clusters} on NVIDIA's Hopper architecture\cite{}, we are unaware of any GPU programming framework\footnote{We are not aware of any prior inter-workgroup level barrier primitive in CUDA, OpenCL, GLSL, HLSL, or Metal.} that provided inter-workgroup synchronization primitives. Instead, the earlier \emph{Scan-then-Propagate} and \emph{Reduce-then-Scan} architectures rely on kernel launches to act as inter-workgroup synchronization points\cite{}. 
 
 The \emph{Chained-Scan} architecture operates in the middle ground between the absence of inter-workgroup synchronization and formal inter-workgroup synchronization support. Rather than relying on explicit synchronization primitives, it leverages a combination of atomic operations and a property of the hardware's workgroup scheduling model known as a \emph{forward progress guarantee}. Atomics enable coherent communication between workgroups, while the \emph{forward progress guarantee} (FPG) ensures that all active workgroups will eventually make progress towards termination, precluding scenarios where a workgroup is scheduled in a manner that starves another workgroup of execution time.
 
 In \emph{Chained-Scan}, workgroups operate within a producer-consumer framework: each workgroup produces the reduction of its assigned work tile, and every workgroup (except the first) must consume the reductions produced by all preceding workgroups to complete the scan. To guarantee that no workgroup begins scanning a tile for which the preceding reduction may be unavailable, \emph{Chained-Scan} assigns work tiles using atomic increment operations rather than assignment based on workgroup index. Once a workgroup acquires its tile, it computes the tile’s reduction, atomically posts the result into global memory, and then waits for preceding reductions to appear in global memory. It is this waiting phase that necessitates FPG. Because workgroups do not execute in physical lockstep (obvious?), some may begin waiting before others have finished their calculations. Without FPG, there is a risk that a workgroup which finishes early could remain indefinitely scheduled in a waiting state, blocking progress by the workgroup responsible for producing the needed reduction and ultimately causing a deadlock. 
 
 As an illustrative example, consider a scan operation which requires exactly two workgroups, running on a GPU with a single multiprocessor. While this multiprocessor can host both workgroups, it contains only a single scheduling unit, which lacks FPG. In this scenario, both workgroups launch and acquire their respective work tiles, but the workgroup responsible for \emph{Work Tile 1} finishes first and begins waiting on \emph{Work Tile 0}. Because the scheduler lacks FPG, it continues scheduling \emph{Work Tile 1}, but because there is only a single scheduling unit, \emph{Work Tile 0} is never scheduled and never completes its reduction. As a result, \emph{Work Tile 1} spins indefinitely until \emph{timeout detection and recovery} (TDR) is triggered, crashing the host program in the process.

\subsection{Why is reliance on Forward Progress Guarantees a Portability Problem?}
 Although NVIDIA formalized FPG down to the subgroup level beginning with the Volta architecture---and FPG was likely already present at the workgroup level on at least NVIDIA's Fermi and AMD's TeraScale2 architectures\cite{}---it remains absent on some contemporary hardware, most notably certain ARM-based chips, including Apple's M-series GPUs\cite{10.1145/3485508}. Our testing shows that, at best, attempting to run \emph{Chained-Scan} without FPG results in mega-bad\footnote{Need to gather data on the distribution of M1 Pro time on Decoupled Lookback instead of the average time. It's also unclear why the test did not trigger Metal's timeout device recovery}, and at worst, risks the afformentioned TDR and subsequent program crash. This risk is further exacerbated by the fact that, no major graphics API currently provides developers an FPG hardware capability query\footnote{To the best of our knowledge D3D12, Vulkan, and Metal do not have a query for FPG}. As a result, when implementing scan operations in a shading environment—where cross-vendor, cross-architecture portability is essential—uncertainty surrounding FPG capability and its catastrophic failure mode compels developers to fall back to the older, slower \emph{Reduce-then-Scan} approach.

\subsection{Earlier Attempts at Portability}

\section{Design}
\subsection{Goals (These subsection headers can be removed later if necessary)}
 This work is guided by two main goals. First, portability: we want to develop a scan implementation that retains the benefits of the \emph{Chained-Scan} architecture but is also suitable for a diverse range of hardware vendors and achitectures, including those without FPG. Second, performance: we aim for near speed-of-light execution to the greatest extent possible allowed by the underlying hardware and programming model. To ground these objectives, we select the WebGPU shading language (WGSL) and the WebGPU standard as our target environment. WGSL is a meta-level shading language which is translated and compiled as necessary for backend graphics APIs---D3D12, Vulkan, and Metal---and as such, WGSL implementations are bound by the minimum capability across all backends. Because WGSL must operate within this limited capability space, it inherently embodies the challenge of portability.

\subsection{Non Goals}
Although our goal is to create a fully portable and highly performant scan implementation, we are constrained by underlying hardwares and programming models. As discussed in more detail in \emph{Limits on the Speed-of-Light}, not all architectures offer atomic operations that are sufficiently fast enough or scheduling models that are sufficeintly fair enough to attain speed-of-light performance, and thus we cannot guarantee such performance. Although we are cognisant of the risks posed by subgroup divergence, subgroup functions in shading languages do not allow explicit divergence control\footnote{For example, CUDA allows developers to explicitly provision subgroup functions with a mask of the threads that will participate in it.}, placing a solution to subgroup divergence issues outside our scope. Furthermore, although real-world scans may require more than 30 bits of space for their reductions, or may involve scanning across multiple struct members, WGSL’s current limitations---lack of fences and 64-bit atomics---lead us to focus on the simplest scenario that fits within the 30-bit range. Lastly, we do not investigate alternative $O(2n)$ scan architectures beyond \emph{Chained-Scan}.

\subsection{Achieving Goals}

\section{Implementation}

\subsection{Decoupled-Fallback}

\begin{algorithm}[htbp]
  \SetAlgoLined
  \KwIn{Input data $X$, threshold $\epsilon$}
  \KwOut{Processed data $Y$}
  Initialize $Y \leftarrow \emptyset$\;
  \ForEach{$x \in X$}{
      \If{$x > \epsilon$}{
          Add $x$ to $Y$\;
      }
  }
  \Return{$Y$}\;
  \caption{Example Algorithm}
  \label{alg:example}
  \end{algorithm}
\section{Experimental Setup}

\section{Results and Analysis}

\section{Discussion}

\subsection{Tradeoffs}

\subsection{Limits on Speed-of-Light Performance}
 There are a number of factors which may preclude speed-of-light performance, but most salient is the fairness of the scheduling model. In \emph{Decoupled-Fallback}, a work tile which is late due to unfairness is indistinguishable from one which is deadlocking, so as a scheduler becomes increasingly unfair, it incurs an increasing number of redundant fallbacks. Consider a hardware with a workgroup occupancy denoted by $o$, and let $f$ represent the probability that a fallback operation occurs at a particular lookback step. Because the number of lookback steps is bounded by $o$, the expected number of fallback operations is approximately $fo$. This results in a factor of $fo$ increase in global memory reads and a factor of $fo\log{n}$ increase in work.
 
 This increased sensitivity to fairness exacerabtes existing issues which negatively impact the performance of previous scan architectures. On hardware that lacks sufficient compute power to be memory-bandwidth bound, the additional work incurred by redundant fallback reductions is particularly deleterious. High atomic update latency further compounds the problem: as inter-workgroup communication time grows, the minimum delay before updates become visible to dependent workgroups also increases. This, in turn, raises the number of lookback steps that may be needed and potentially leads to redundant fallbacks.

 Fairness has been a priority in NVIDIA hardware since at least the Tesla architecture\cite{}, but the extent to which it is implemented and prioritized by other vendors remains unclear. While our results on the least fair hardware demonstrate that \emph{Decoupled-Fallback} remains performant, this performance may not extend to scans involving more complex binary reduction operators or less capable hardware.
\begin{acks}
\end{acks}

\bibliographystyle{ACM-Reference-Format}
\bibliography{bib}

\appendix

\section{Research Methods}

\end{document}
\endinput
